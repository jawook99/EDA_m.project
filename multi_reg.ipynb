{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "084824cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WD] c:\\Users\\USER\\Desktop\\Project_Data\n",
      "[auto-drop aliasing] ['사고유형', '도로형태', '기상상태']\n",
      "[sample] train rows: 86,162 / 522,926\n",
      "[BASE] AIC=335722.5, n=86162 -> MODEL_BASE_results.csv\n",
      "[FULL] AIC=335631.9, n=86162 -> MODEL_FULL_results.csv, INTER_LIST_used.csv\n",
      "[COMPARE] LR_TEST_BASE_vs_FULL.csv saved\n",
      "  LR=200.55, df=55, p=1.974e-18, ΔAIC=-90.55, ΔBIC=432.80\n",
      "[PIVOT] saved -> PIVOT_pred_mean_법규위반x가해자차종.csv, PIVOT_mult_%_법규위반x가해자차종.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# pip install pandas statsmodels scipy\n",
    "\n",
    "import os, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import chi2\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ========== 설정 ==========\n",
    "PATH = r\"./5_tt_10년평균_통합지수.csv\"  # 필요시 연평균 파일로 교체\n",
    "TARGET = \"통합지수\"\n",
    "\n",
    "# 후보 범주/수치 변수(파일에 없으면 자동 제외됨)\n",
    "CANDIDATE_CATS = [\n",
    "    \"사고형태\",\"법규위반\",\"사고요일\",\"일광상태\",\n",
    "    \"사고유형\",\"도로형태\",\"가해자차종\",\"일당운전자경력\",\n",
    "    \"피해운전자 연령대\",\"가해운전자 연령대\",\"가해자성별\",\"피해운전자 성별\",\"기상상태\"\n",
    "]\n",
    "CANDIDATE_NUMS = [\"사고시각\",\"가해자나이\",\"피해자나이\"]  # 존재하면 사용\n",
    "\n",
    "# 포함할 상호작용(우선순위 높은 2~3개 추천)\n",
    "SELECTED_INTERACTIONS = [\n",
    "    (\"법규위반\",\"가해자차종\"),\n",
    "    (\"사고형태\",\"도로형태\"),\n",
    "    (\"사고유형\",\"사고형태\"),\n",
    "    # 필요시 (\"사고요일\",\"일광상태\") 도 추가 가능\n",
    "]\n",
    "\n",
    "# 안전/성능 파라미터\n",
    "MIN_LEVEL_N = 100          # 희소 레벨은 '기타'로 묶기\n",
    "TOPK_LEVELS_INTER = 10     # 상호작용 변수 각자 상위 K레벨만 사용(교차항 폭발 방지)\n",
    "MAX_TRAIN_ROWS    = 200_000  # **최대 학습 행 수 cap** (메모리 부족시 더 낮추기)\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# ========== 유틸 ==========\n",
    "def read_csv_any(path):\n",
    "    for enc in [\"utf-8\",\"cp949\",\"utf-8-sig\"]:\n",
    "        try: return pd.read_csv(path, encoding=enc)\n",
    "        except Exception: pass\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def uniq(seq): return list(dict.fromkeys(list(seq)))\n",
    "\n",
    "def collapse_rare_levels(df, col, min_n=100, other=\"기타\"):\n",
    "    vc = df[col].value_counts(dropna=True)\n",
    "    rare = vc[vc < min_n].index\n",
    "    if len(rare) > 0:\n",
    "        df[col] = df[col].where(~df[col].isin(rare), other)\n",
    "    return df\n",
    "\n",
    "def keep_topk_levels(df, col, k=TOPK_LEVELS_INTER, other=\"기타\"):\n",
    "    vc = df[col].value_counts(dropna=True)\n",
    "    keep = set(vc.index[:k])\n",
    "    df[col] = df[col].where(df[col].isin(keep), other)\n",
    "    return df\n",
    "\n",
    "def to_hour(s):\n",
    "    x = pd.to_numeric(s, errors=\"coerce\")\n",
    "    if x.dropna().size and x.dropna().max() > 100:\n",
    "        return (x // 100).clip(0, 23)\n",
    "    return x\n",
    "\n",
    "def choose_cluster_col(df):\n",
    "    # 군집SE 후보 자동 선택(부적합하면 HC3)\n",
    "    for c in [\"사고장소\",\"지점ID\",\"IDX\",\"시도\"]:\n",
    "        if c in df.columns:\n",
    "            g = df[c].fillna(\"__NA__\")\n",
    "            if g.nunique() >= 5 and g.nunique() <= 0.95*len(g):\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def cov_from(df, cluster_col):\n",
    "    if cluster_col and (cluster_col in df.columns):\n",
    "        g = df[cluster_col].fillna(\"__NA__\")\n",
    "        if g.nunique() >= 5 and g.nunique() <= 0.95*len(g) and (g.value_counts().max() >= 2):\n",
    "            return \"cluster\", {\"groups\": g.to_numpy()}\n",
    "    return \"HC3\", {}\n",
    "\n",
    "def drop_near_alias(df, cat_cols, thresh=0.98):\n",
    "    # 서로 거의 1:1로 겹치는 축 자동 제거(near-aliasing)\n",
    "    keep, dropped = [], []\n",
    "    for c in cat_cols:\n",
    "        if c not in df.columns: continue\n",
    "        if df[c].nunique() <= 1:\n",
    "            dropped.append(c); \n",
    "            continue\n",
    "        to_drop = False\n",
    "        for k in keep:\n",
    "            if k not in df.columns: continue\n",
    "            tab = pd.crosstab(df[k], df[c])\n",
    "            if tab.values.sum() == 0: continue\n",
    "            col_acc = (tab.div(tab.sum(0), 0).max(0)).mean()\n",
    "            row_acc = (tab.div(tab.sum(1), 0).max(1)).mean()\n",
    "            if max(col_acc, row_acc) >= thresh:\n",
    "                to_drop = True; break\n",
    "        if to_drop: dropped.append(c)\n",
    "        else: keep.append(c)\n",
    "    return keep, dropped\n",
    "\n",
    "# ---------- 결측/전처리 ----------\n",
    "def impute_all(df, cat_cols, num_cols):\n",
    "    df = df.copy()\n",
    "    # 범주 결측 → '미상'\n",
    "    for c in cat_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(\"object\").fillna(\"미상\")\n",
    "    # 수치 결측/이상치 대치\n",
    "    for c in num_cols:\n",
    "        if c not in df.columns: \n",
    "            continue\n",
    "        s = df[c]\n",
    "        if c == \"사고시각\": s = to_hour(s)\n",
    "        s = pd.to_numeric(s, errors=\"coerce\")\n",
    "        med = s.median()\n",
    "        if pd.isna(med): med = 0.0\n",
    "        df[c] = s.fillna(med)\n",
    "    return df\n",
    "\n",
    "# ---------- 층화 샘플링(cap) ----------\n",
    "def stratified_cap(df, inter_pairs, cat_base, max_rows=MAX_TRAIN_ROWS, seed=RANDOM_SEED):\n",
    "    if len(df) <= max_rows:\n",
    "        return df.copy()\n",
    "\n",
    "    # 층 변수 = (선택 상호작용에 등장하는 모든 범주축) 있으면 그걸로,\n",
    "    # 없으면 cat_base 앞의 2개로\n",
    "    strata_cols = uniq([c for pair in inter_pairs for c in pair if c in cat_base])\n",
    "    if len(strata_cols) == 0:\n",
    "        strata_cols = cat_base[:2] if len(cat_base) >= 2 else cat_base\n",
    "\n",
    "    # 상호작용 관측 보호를 위해 TopK 레벨 제한 후 층화\n",
    "    tmp = df.copy()\n",
    "    for c in strata_cols:\n",
    "        if c in tmp.columns:\n",
    "            tmp = keep_topk_levels(tmp, c, TOPK_LEVELS_INTER)\n",
    "    g = tmp.groupby(strata_cols, dropna=False, sort=False)\n",
    "\n",
    "    cap_per = max(50, int(max_rows / max(1, g.ngroups)))\n",
    "    parts = []\n",
    "    rng = np.random.default_rng(seed)\n",
    "    for _, sub in g:\n",
    "        n = len(sub)\n",
    "        if n <= cap_per:\n",
    "            parts.append(sub)\n",
    "        else:\n",
    "            parts.append(sub.sample(cap_per, random_state=seed))\n",
    "    out = pd.concat(parts, axis=0)\n",
    "    # cap 초과시 랜덤 추가 샘플 조정\n",
    "    if len(out) > max_rows:\n",
    "        out = out.sample(max_rows, random_state=seed)\n",
    "    return out\n",
    "\n",
    "# ---------- 설계행렬(주효과 + 선택 상호작용) ----------\n",
    "def build_X(df, cat_cols, num_cols, inter_pairs):\n",
    "    df = df.copy()\n",
    "\n",
    "    # 상호작용 변수는 Top-K 레벨로 제한 (교차항 폭발 방지)\n",
    "    for a, b in inter_pairs:\n",
    "        if a in df.columns: df = keep_topk_levels(df, a, TOPK_LEVELS_INTER)\n",
    "        if b in df.columns: df = keep_topk_levels(df, b, TOPK_LEVELS_INTER)\n",
    "\n",
    "    # 더미 생성\n",
    "    cats_for_dummies = [c for c in cat_cols if c in df.columns]\n",
    "    Xc = pd.get_dummies(df[cats_for_dummies], drop_first=True)\n",
    "    Xc = Xc.loc[:, ~Xc.columns.duplicated()]\n",
    "\n",
    "    # 선택한 페어만 상호작용 더미 추가\n",
    "    for a, b in inter_pairs:\n",
    "        A = [c for c in Xc.columns if c.startswith(a + \"_\")]\n",
    "        B = [c for c in Xc.columns if c.startswith(b + \"_\")]\n",
    "        for ca in A:\n",
    "            ca_s = Xc[ca]\n",
    "            for cb in B:\n",
    "                Xc[f\"{ca}:{cb}\"] = ca_s * Xc[cb]\n",
    "\n",
    "    # 수치와 결합\n",
    "    Xn = df[num_cols].copy() if num_cols else pd.DataFrame(index=df.index)\n",
    "    X = pd.concat([Xn.astype(float), Xc.astype(float)], axis=1)\n",
    "\n",
    "    # 제로분산 열 제거\n",
    "    if len(X.columns):\n",
    "        nonzero = X.std(axis=0).replace(0, np.nan).notna()\n",
    "        X = X.loc[:, nonzero]\n",
    "\n",
    "    # inf → NaN 처리 (뒤 단계에서 일괄 제거)\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # 상수항\n",
    "    X = sm.add_constant(X, has_constant=\"add\")\n",
    "    return X\n",
    "\n",
    "def align_and_clean(X, y_raw, cov_kwds=None):\n",
    "    y = pd.to_numeric(pd.Series(y_raw), errors=\"coerce\").values\n",
    "    na_mask = X.isna().any(axis=1).values\n",
    "    mask = np.isfinite(y) & (~na_mask)\n",
    "\n",
    "    Xc = X.loc[mask]\n",
    "    yc = y[mask]\n",
    "\n",
    "    cov_kwds_out = cov_kwds\n",
    "    if cov_kwds is not None and \"groups\" in cov_kwds:\n",
    "        cov_kwds_out = dict(cov_kwds)\n",
    "        cov_kwds_out[\"groups\"] = np.asarray(cov_kwds[\"groups\"])[mask]\n",
    "\n",
    "    dropped = int((~mask).sum())\n",
    "    if dropped > 0:\n",
    "        print(f\"[clean] dropped {dropped} rows due to NaN/inf in X or y\")\n",
    "\n",
    "    if len(Xc) == 0:\n",
    "        raise ValueError(\"No rows left after cleaning NaN/inf. Check inputs.\")\n",
    "    return Xc, yc, cov_kwds_out\n",
    "\n",
    "def fit_glm_gamma(X, y, cov_type, cov_kwds):\n",
    "    # 1차: 일반 적합 → 실패 시 초소형 릿지\n",
    "    try:\n",
    "        return sm.GLM(y, X, family=sm.families.Gamma(link=sm.families.links.log()))\\\n",
    "                .fit(cov_type=cov_type, cov_kwds=cov_kwds)\n",
    "    except Exception:\n",
    "        return sm.GLM(y, X, family=sm.families.Gamma(link=sm.families.links.log()))\\\n",
    "                .fit_regularized(alpha=1e-6, L1_wt=0.0)\n",
    "\n",
    "def export_coef_table(res, path):\n",
    "    out = pd.DataFrame({\n",
    "        \"term\": res.params.index,\n",
    "        \"coef\": res.params.values,\n",
    "        \"std_err\": getattr(res, \"bse\", pd.Series(index=res.params.index, dtype=float)).reindex(res.params.index).values,\n",
    "        \"p_value\": getattr(res, \"pvalues\", pd.Series(index=res.params.index, dtype=float)).reindex(res.params.index).values\n",
    "    })\n",
    "    out[\"mult_effect_%\"] = (np.exp(out[\"coef\"]) - 1.0) * 100.0\n",
    "    out.to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "    return out\n",
    "\n",
    "# ========== 0) 데이터 로드 & 전처리 ==========\n",
    "print(\"[WD]\", os.getcwd())\n",
    "df = read_csv_any(PATH)\n",
    "assert TARGET in df.columns, f\"'{TARGET}' 컬럼이 있어야 합니다.\"\n",
    "\n",
    "# Y>0만 사용\n",
    "df = df[df[TARGET].notna() & (pd.to_numeric(df[TARGET], errors=\"coerce\") > 0)].copy()\n",
    "df[TARGET] = pd.to_numeric(df[TARGET], errors=\"coerce\")\n",
    "\n",
    "# 누수 방지: 이름에 이런 단어가 들어간 컬럼은 피처에서 제외\n",
    "LEAKS = [\"통합지수\",\"다발도\",\"심각도\",\"사고건수\",\"건수\",\"사망사고건수\",\"중상사고건수\",\"경상사고건수\"]\n",
    "is_safe = lambda c: not any(w in str(c) for w in LEAKS)\n",
    "\n",
    "cat_base = [c for c in CANDIDATE_CATS if c in df.columns and is_safe(c)]\n",
    "num_base = [c for c in CANDIDATE_NUMS if c in df.columns and is_safe(c)]\n",
    "\n",
    "# 희소 레벨 정리\n",
    "for c in cat_base:\n",
    "    df = collapse_rare_levels(df, c, min_n=MIN_LEVEL_N)\n",
    "\n",
    "# near-aliasing 제거\n",
    "cat_base, dropped_alias = drop_near_alias(df, cat_base, thresh=0.98)\n",
    "if dropped_alias:\n",
    "    print(\"[auto-drop aliasing]\", dropped_alias)\n",
    "\n",
    "# 상호작용 리스트 중 존재하는 것만\n",
    "SELECTED_INTERACTIONS = [(a,b) for (a,b) in SELECTED_INTERACTIONS if (a in cat_base and b in cat_base)]\n",
    "\n",
    "# 결측 대치\n",
    "df = impute_all(df, cat_base, num_base)\n",
    "\n",
    "# ========== (NEW) 학습 표본 층화 샘플링 ==========\n",
    "df_train = stratified_cap(df, SELECTED_INTERACTIONS, cat_base, max_rows=MAX_TRAIN_ROWS, seed=RANDOM_SEED)\n",
    "print(f\"[sample] train rows: {len(df_train):,} / {len(df):,}\")\n",
    "\n",
    "# ========== 1) 베이스(주효과) 모형 ==========\n",
    "cluster_col = choose_cluster_col(df_train)\n",
    "cov_type0, cov_kw0 = cov_from(df_train, cluster_col)\n",
    "\n",
    "X_base = build_X(df_train, cat_base, num_base, inter_pairs=[])  # 상호작용 없음\n",
    "y_raw = df_train[TARGET].values\n",
    "X_base, y_base, cov_kw0 = align_and_clean(X_base, y_raw, cov_kw0)\n",
    "fit_base = fit_glm_gamma(X_base, y_base, cov_type0, cov_kw0)\n",
    "export_coef_table(fit_base, \"MODEL_BASE_results.csv\")\n",
    "print(f\"[BASE] AIC={fit_base.aic:.1f}, n={int(fit_base.nobs)} -> MODEL_BASE_results.csv\")\n",
    "\n",
    "# ========== 2) 풀(주효과 + 선택 상호작용) 모형 ==========\n",
    "cov_typeF, cov_kwF = cov_from(df_train, cluster_col)\n",
    "X_full = build_X(df_train, cat_base, num_base, inter_pairs=SELECTED_INTERACTIONS)\n",
    "X_full, y_full, cov_kwF = align_and_clean(X_full, y_raw, cov_kwF)\n",
    "fit_full = fit_glm_gamma(X_full, y_full, cov_typeF, cov_kwF)\n",
    "export_coef_table(fit_full, \"MODEL_FULL_results.csv\")\n",
    "pd.DataFrame({\"interaction_pairs\":[f\"{a} × {b}\" for a,b in SELECTED_INTERACTIONS]}).to_csv(\n",
    "    \"INTER_LIST_used.csv\", index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"[FULL] AIC={fit_full.aic:.1f}, n={int(fit_full.nobs)} -> MODEL_FULL_results.csv, INTER_LIST_used.csv\")\n",
    "\n",
    "# ========== 3) BASE vs FULL 개선도(LR 테스트 & ΔAIC/BIC) ==========\n",
    "# 동일 표본/상수항 기준으로 비교\n",
    "ll0, ll1 = fit_base.llf, fit_full.llf\n",
    "df0, df1 = fit_base.df_model, fit_full.df_model\n",
    "LR = 2*(ll1 - ll0)\n",
    "ddf = max(int(df1 - df0), 1)\n",
    "p = chi2.sf(LR, ddf)\n",
    "delta_AIC = fit_full.aic - fit_base.aic\n",
    "delta_BIC = fit_full.bic - fit_base.bic\n",
    "\n",
    "pd.DataFrame([{\n",
    "    \"LR_stat\": LR, \"df_diff\": ddf, \"p_value\": p,\n",
    "    \"AIC_base\": fit_base.aic, \"AIC_full\": fit_full.aic, \"ΔAIC\": delta_AIC,\n",
    "    \"BIC_base\": fit_base.bic, \"BIC_full\": fit_full.bic, \"ΔBIC\": delta_BIC,\n",
    "    \"n_train\": int(fit_full.nobs)\n",
    "}]).to_csv(\"LR_TEST_BASE_vs_FULL.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"[COMPARE] LR_TEST_BASE_vs_FULL.csv saved\")\n",
    "print(f\"  LR={LR:.2f}, df={ddf}, p={p:.3e}, ΔAIC={delta_AIC:.2f}, ΔBIC={delta_BIC:.2f}\")\n",
    "\n",
    "# ========== 4) 대표 상호작용(첫 번째) 피벗 예측 ==========\n",
    "if SELECTED_INTERACTIONS:\n",
    "    a, b = SELECTED_INTERACTIONS[0]\n",
    "    # 대표값(연속=중앙값, 범주=최빈값)으로 다른 공변량 고정 (학습표본 기준)\n",
    "    rep = {}\n",
    "    for c in num_base:\n",
    "        if c in df_train.columns:\n",
    "            rep[c] = pd.to_numeric(df_train[c], errors=\"coerce\").median()\n",
    "            if pd.isna(rep[c]): rep[c] = 0.0\n",
    "    for c in cat_base:\n",
    "        if c not in [a,b] and c in df_train.columns:\n",
    "            mode_series = df_train[c].mode(dropna=True)\n",
    "            rep[c] = mode_series.iloc[0] if len(mode_series) else \"미상\"\n",
    "\n",
    "    # 그리드 만들기: a,b의 모든(TopK 후) 레벨 조합\n",
    "    df_ab = df_train[[a,b]].copy()\n",
    "    df_ab = keep_topk_levels(df_ab, a, TOPK_LEVELS_INTER)\n",
    "    df_ab = keep_topk_levels(df_ab, b, TOPK_LEVELS_INTER)\n",
    "    A_levels = df_ab[a].dropna().unique().tolist()\n",
    "    B_levels = df_ab[b].dropna().unique().tolist()\n",
    "\n",
    "    grid = [{a: av, b: bv, **rep} for av in A_levels for bv in B_levels]\n",
    "    new = pd.DataFrame(grid, columns=uniq([a,b] + list(rep.keys())))\n",
    "\n",
    "    # 예측용 설계행렬(훈련 X_full과 동일 규칙 + 열정렬)\n",
    "    X_new = build_X(new, cat_base, num_base, inter_pairs=SELECTED_INTERACTIONS)\n",
    "    X_new = X_new.reindex(columns=fit_full.model.exog_names, fill_value=0.0)\n",
    "\n",
    "    pred = fit_full.get_prediction(exog=X_new).summary_frame()\n",
    "    new[\"pred_mean\"] = pred[\"mean\"]\n",
    "    new[\"pred_low\"]  = pred[\"mean_ci_lower\"]\n",
    "    new[\"pred_high\"] = pred[\"mean_ci_upper\"]\n",
    "\n",
    "    # 피벗: 예측 평균\n",
    "    pivot = new.pivot(index=a, columns=b, values=\"pred_mean\")\n",
    "    pivot.to_csv(f\"PIVOT_pred_mean_{a}x{b}.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "    # 배수효과(%): pivot의 [0,0] 기준\n",
    "    base_val = pivot.iloc[0,0]\n",
    "    mult = (pivot / base_val - 1.0) * 100.0\n",
    "    mult.to_csv(f\"PIVOT_mult_%_{a}x{b}.csv\", encoding=\"utf-8-sig\")\n",
    "    print(f\"[PIVOT] saved -> PIVOT_pred_mean_{a}x{b}.csv, PIVOT_mult_%_{a}x{b}.csv\")\n",
    "else:\n",
    "    print(\"[PIVOT] 상호작용이 지정되지 않아 피벗 생략\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "282d79e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WD] c:\\Users\\USER\\Desktop\\Project_Data\n",
      "[auto-drop aliasing] ['사고유형', '도로형태', '기상상태']\n",
      "[sample] train rows: 86,162 / 522,926\n",
      "[BASE] AIC=335722.5, n=86162 -> MODEL_BASE_results.csv\n",
      "[FULL] AIC=335631.9, n=86162 -> MODEL_FULL_results.csv, INTER_LIST_used.csv\n",
      "[COMPARE] LR_TEST_BASE_vs_FULL.csv saved\n",
      "  LR=200.55, df=55, p=1.974e-18, ΔAIC=-90.55, ΔBIC=432.80\n",
      "[PIVOT] saved -> PIVOT_pred_mean_법규위반x가해자차종.csv, PIVOT_mult_%_법규위반x가해자차종.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# pip install pandas statsmodels scipy\n",
    "\n",
    "import os, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import chi2\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ========== 설정 ==========\n",
    "PATH = r\"./5_tt_10년평균_통합지수.csv\"  # 필요시 연평균 파일로 교체\n",
    "TARGET = \"통합지수\"\n",
    "\n",
    "# 후보 범주/수치 변수(파일에 없으면 자동 제외됨)\n",
    "CANDIDATE_CATS = [\n",
    "    \"사고형태\",\"법규위반\",\"사고요일\",\"일광상태\",\n",
    "    \"사고유형\",\"도로형태\",\"가해자차종\",\"일당운전자경력\",\n",
    "    \"피해운전자 연령대\",\"가해운전자 연령대\",\"가해자성별\",\"피해운전자 성별\",\"기상상태\"\n",
    "]\n",
    "CANDIDATE_NUMS = [\"사고시각\",\"가해자나이\",\"피해자나이\"]  # 존재하면 사용\n",
    "\n",
    "# 포함할 상호작용(우선순위 높은 2~3개 추천) — 바꿔도 안전하게 동작\n",
    "SELECTED_INTERACTIONS = [\n",
    "    (\"법규위반\",\"가해자차종\"),\n",
    "    (\"사고형태\",\"도로형태\"),\n",
    "    (\"사고유형\",\"사고형태\"),\n",
    "    # 필요시 (\"사고요일\",\"일광상태\") 도 추가 가능\n",
    "]\n",
    "\n",
    "# 안전/성능 파라미터\n",
    "MIN_LEVEL_N         = 100      # 희소 레벨은 '기타'로 묶기\n",
    "TOPK_LEVELS_INTER   = 10       # 상호작용 변수 각자 상위 K레벨만 사용(교차항 폭발 방지)\n",
    "MAX_TRAIN_ROWS      = 200_000  # 학습 행 수 cap (메모리 빠듯하면 ↓)\n",
    "MAX_INTER_DUMMIES   = 400      # 상호작용 더미 칼럼 수 안전 캡 (drop_first 기준 (la-1)*(lb-1))\n",
    "RANDOM_SEED         = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# ========== 유틸 ==========\n",
    "def read_csv_any(path):\n",
    "    for enc in [\"utf-8\",\"cp949\",\"utf-8-sig\"]:\n",
    "        try: return pd.read_csv(path, encoding=enc)\n",
    "        except Exception: pass\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def uniq(seq): return list(dict.fromkeys(list(seq)))\n",
    "\n",
    "def collapse_rare_levels(df, col, min_n=100, other=\"기타\"):\n",
    "    vc = df[col].value_counts(dropna=True)\n",
    "    rare = vc[vc < min_n].index\n",
    "    if len(rare) > 0:\n",
    "        df[col] = df[col].where(~df[col].isin(rare), other)\n",
    "    return df\n",
    "\n",
    "def keep_topk_levels(df, col, k=TOPK_LEVELS_INTER, other=\"기타\"):\n",
    "    vc = df[col].value_counts(dropna=True)\n",
    "    keep = set(vc.index[:k])\n",
    "    df[col] = df[col].where(df[col].isin(keep), other)\n",
    "    return df\n",
    "\n",
    "def to_hour(s):\n",
    "    x = pd.to_numeric(s, errors=\"coerce\")\n",
    "    if x.dropna().size and x.dropna().max() > 100:\n",
    "        return (x // 100).clip(0, 23)\n",
    "    return x\n",
    "\n",
    "def choose_cluster_col(df):\n",
    "    # 군집SE 후보 자동 선택(부적합하면 HC3)\n",
    "    for c in [\"사고장소\",\"지점ID\",\"IDX\",\"시도\"]:\n",
    "        if c in df.columns:\n",
    "            g = df[c].fillna(\"__NA__\")\n",
    "            if g.nunique() >= 5 and g.nunique() <= 0.95*len(g):\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def cov_from(df, cluster_col):\n",
    "    if cluster_col and (cluster_col in df.columns):\n",
    "        g = df[cluster_col].fillna(\"__NA__\")\n",
    "        if g.nunique() >= 5 and g.nunique() <= 0.95*len(g) and (g.value_counts().max() >= 2):\n",
    "            return \"cluster\", {\"groups\": g.to_numpy()}\n",
    "    return \"HC3\", {}\n",
    "\n",
    "def drop_near_alias(df, cat_cols, thresh=0.98):\n",
    "    # 서로 거의 1:1로 겹치는 축 자동 제거(near-aliasing)\n",
    "    keep, dropped = [], []\n",
    "    for c in cat_cols:\n",
    "        if c not in df.columns: continue\n",
    "        if df[c].nunique() <= 1:\n",
    "            dropped.append(c); \n",
    "            continue\n",
    "        to_drop = False\n",
    "        for k in keep:\n",
    "            if k not in df.columns: continue\n",
    "            tab = pd.crosstab(df[k], df[c])\n",
    "            if tab.values.sum() == 0: continue\n",
    "            col_acc = (tab.div(tab.sum(0), 0).max(0)).mean()\n",
    "            row_acc = (tab.div(tab.sum(1), 0).max(1)).mean()\n",
    "            if max(col_acc, row_acc) >= thresh:\n",
    "                to_drop = True; break\n",
    "        if to_drop: dropped.append(c)\n",
    "        else: keep.append(c)\n",
    "    return keep, dropped\n",
    "\n",
    "# ---------- (보강1) 상호작용 유효성 점검 ----------\n",
    "def validate_and_filter_interactions(df, cat_base, pairs, min_levels=2):\n",
    "    ok = []\n",
    "    for a, b in pairs:\n",
    "        if a not in cat_base or b not in cat_base:\n",
    "            print(f\"[skip] '{a}×{b}': not in cat_base\")\n",
    "            continue\n",
    "        la, lb = df[a].nunique(), df[b].nunique()\n",
    "        if la < min_levels or lb < min_levels:\n",
    "            print(f\"[skip] '{a}×{b}': insufficient levels ({la},{lb})\")\n",
    "            continue\n",
    "        ok.append((a, b))\n",
    "    return ok\n",
    "\n",
    "# ---------- (보강3) 상호작용 더미 칼럼 수 캡 ----------\n",
    "def cap_interaction_levels_for_pair(df, a, b, max_inter_dummies=MAX_INTER_DUMMIES):\n",
    "    if a not in df.columns or b not in df.columns:\n",
    "        return df\n",
    "    la, lb = df[a].nunique(), df[b].nunique()\n",
    "    tgt_a, tgt_b = la, lb\n",
    "    # drop_first 기준 더미 수 ≈ (la-1)*(lb-1)\n",
    "    while (tgt_a-1)*(tgt_b-1) > max_inter_dummies and (tgt_a > 2 or tgt_b > 2):\n",
    "        if tgt_a >= tgt_b and tgt_a > 2:\n",
    "            tgt_a -= 1\n",
    "        elif tgt_b > 2:\n",
    "            tgt_b -= 1\n",
    "        else:\n",
    "            break\n",
    "    if tgt_a < la: df = keep_topk_levels(df, a, tgt_a)\n",
    "    if tgt_b < lb: df = keep_topk_levels(df, b, tgt_b)\n",
    "    return df\n",
    "\n",
    "# ---------- 결측/전처리 ----------\n",
    "def impute_all(df, cat_cols, num_cols):\n",
    "    df = df.copy()\n",
    "    # 범주 결측 → '미상'\n",
    "    for c in cat_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(\"object\").fillna(\"미상\")\n",
    "    # 수치 결측/이상치 대치\n",
    "    for c in num_cols:\n",
    "        if c not in df.columns: \n",
    "            continue\n",
    "        s = df[c]\n",
    "        if c == \"사고시각\": s = to_hour(s)\n",
    "        s = pd.to_numeric(s, errors=\"coerce\")\n",
    "        med = s.median()\n",
    "        if pd.isna(med): med = 0.0\n",
    "        df[c] = s.fillna(med)\n",
    "    return df\n",
    "\n",
    "# ---------- 층화 샘플링(cap) ----------\n",
    "def stratified_cap(df, inter_pairs, cat_base, max_rows=MAX_TRAIN_ROWS, seed=RANDOM_SEED):\n",
    "    if len(df) <= max_rows:\n",
    "        return df.copy()\n",
    "    strata_cols = uniq([c for pair in inter_pairs for c in pair if c in cat_base])\n",
    "    if len(strata_cols) == 0:\n",
    "        strata_cols = cat_base[:2] if len(cat_base) >= 2 else cat_base\n",
    "    tmp = df.copy()\n",
    "    for c in strata_cols:\n",
    "        if c in tmp.columns:\n",
    "            tmp = keep_topk_levels(tmp, c, TOPK_LEVELS_INTER)\n",
    "    g = tmp.groupby(strata_cols, dropna=False, sort=False)\n",
    "    cap_per = max(50, int(max_rows / max(1, g.ngroups)))\n",
    "    parts = []\n",
    "    for _, sub in g:\n",
    "        n = len(sub)\n",
    "        parts.append(sub.sample(cap_per, random_state=seed) if n > cap_per else sub)\n",
    "    out = pd.concat(parts, axis=0)\n",
    "    if len(out) > max_rows:\n",
    "        out = out.sample(max_rows, random_state=seed)\n",
    "    return out\n",
    "\n",
    "# ---------- 설계행렬(주효과 + 선택 상호작용) ----------\n",
    "def build_X(df, cat_cols, num_cols, inter_pairs):\n",
    "    df = df.copy()\n",
    "\n",
    "    # 상호작용 변수: Top-K 제한 후, 칼럼 수 캡 적용\n",
    "    for a, b in inter_pairs:\n",
    "        if a in df.columns: df = keep_topk_levels(df, a, TOPK_LEVELS_INTER)\n",
    "        if b in df.columns: df = keep_topk_levels(df, b, TOPK_LEVELS_INTER)\n",
    "        df = cap_interaction_levels_for_pair(df, a, b, max_inter_dummies=MAX_INTER_DUMMIES)\n",
    "\n",
    "    # 더미 생성\n",
    "    cats_for_dummies = [c for c in cat_cols if c in df.columns]\n",
    "    Xc = pd.get_dummies(df[cats_for_dummies], drop_first=True)\n",
    "    Xc = Xc.loc[:, ~Xc.columns.duplicated()]\n",
    "\n",
    "    # 선택한 페어만 상호작용 더미 추가\n",
    "    for a, b in inter_pairs:\n",
    "        A = [c for c in Xc.columns if c.startswith(a + \"_\")]\n",
    "        B = [c for c in Xc.columns if c.startswith(b + \"_\")]\n",
    "        for ca in A:\n",
    "            ca_s = Xc[ca]\n",
    "            for cb in B:\n",
    "                Xc[f\"{ca}:{cb}\"] = ca_s * Xc[cb]\n",
    "\n",
    "    # 수치와 결합\n",
    "    Xn = df[num_cols].copy() if num_cols else pd.DataFrame(index=df.index)\n",
    "    X = pd.concat([Xn.astype(float), Xc.astype(float)], axis=1)\n",
    "\n",
    "    # 제로분산 열 제거\n",
    "    if len(X.columns):\n",
    "        nonzero = X.std(axis=0).replace(0, np.nan).notna()\n",
    "        X = X.loc[:, nonzero]\n",
    "\n",
    "    # inf → NaN 처리 (뒤 단계에서 일괄 제거)\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # 상수항\n",
    "    X = sm.add_constant(X, has_constant=\"add\")\n",
    "    return X\n",
    "\n",
    "def align_and_clean(X, y_raw, cov_kwds=None):\n",
    "    y = pd.to_numeric(pd.Series(y_raw), errors=\"coerce\").values\n",
    "    na_mask = X.isna().any(axis=1).values\n",
    "    mask = np.isfinite(y) & (~na_mask)\n",
    "    Xc = X.loc[mask]\n",
    "    yc = y[mask]\n",
    "    cov_kwds_out = cov_kwds\n",
    "    if cov_kwds is not None and \"groups\" in cov_kwds:\n",
    "        cov_kwds_out = dict(cov_kwds)\n",
    "        cov_kwds_out[\"groups\"] = np.asarray(cov_kwds[\"groups\"])[mask]\n",
    "    dropped = int((~mask).sum())\n",
    "    if dropped > 0:\n",
    "        print(f\"[clean] dropped {dropped} rows due to NaN/inf in X or y\")\n",
    "    if len(Xc) == 0:\n",
    "        raise ValueError(\"No rows left after cleaning NaN/inf. Check inputs.\")\n",
    "    return Xc, yc, cov_kwds_out\n",
    "\n",
    "def fit_glm_gamma(X, y, cov_type, cov_kwds):\n",
    "    # 1차: 일반 적합 → 실패 시 초소형 릿지\n",
    "    try:\n",
    "        return sm.GLM(y, X, family=sm.families.Gamma(link=sm.families.links.log()))\\\n",
    "                .fit(cov_type=cov_type, cov_kwds=cov_kwds)\n",
    "    except Exception:\n",
    "        return sm.GLM(y, X, family=sm.families.Gamma(link=sm.families.links.log()))\\\n",
    "                .fit_regularized(alpha=1e-6, L1_wt=0.0)\n",
    "\n",
    "def export_coef_table(res, path):\n",
    "    out = pd.DataFrame({\n",
    "        \"term\": res.params.index,\n",
    "        \"coef\": res.params.values,\n",
    "        \"std_err\": getattr(res, \"bse\", pd.Series(index=res.params.index, dtype=float)).reindex(res.params.index).values,\n",
    "        \"p_value\": getattr(res, \"pvalues\", pd.Series(index=res.params.index, dtype=float)).reindex(res.params.index).values\n",
    "    })\n",
    "    out[\"mult_effect_%\"] = (np.exp(out[\"coef\"]) - 1.0) * 100.0\n",
    "    out.to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "    return out\n",
    "\n",
    "# ---------- (보강2) 안전 예측기 ----------\n",
    "def safe_predict_mean(res, X_new):\n",
    "    if hasattr(res, \"get_prediction\"):\n",
    "        try:\n",
    "            sf = res.get_prediction(exog=X_new).summary_frame()\n",
    "            mean = sf[\"mean\"]\n",
    "            low  = sf[\"mean_ci_lower\"] if \"mean_ci_lower\" in sf else None\n",
    "            high = sf[\"mean_ci_upper\"] if \"mean_ci_upper\" in sf else None\n",
    "            return mean, low, high\n",
    "        except Exception:\n",
    "            pass\n",
    "    # fallback: 수동 계산 (CI 생략)\n",
    "    lin = np.dot(X_new, res.params)\n",
    "    mu = res.model.family.fitted(lin)   # log 링크 → exp 변환\n",
    "    return pd.Series(mu, index=X_new.index), None, None\n",
    "\n",
    "# ========== 0) 데이터 로드 & 전처리 ==========\n",
    "print(\"[WD]\", os.getcwd())\n",
    "df = read_csv_any(PATH)\n",
    "assert TARGET in df.columns, f\"'{TARGET}' 컬럼이 있어야 합니다.\"\n",
    "\n",
    "# Y>0만 사용\n",
    "df = df[df[TARGET].notna() & (pd.to_numeric(df[TARGET], errors=\"coerce\") > 0)].copy()\n",
    "df[TARGET] = pd.to_numeric(df[TARGET], errors=\"coerce\")\n",
    "\n",
    "# 누수 방지: 이름에 이런 단어가 들어간 컬럼은 피처에서 제외\n",
    "LEAKS = [\"통합지수\",\"다발도\",\"심각도\",\"사고건수\",\"건수\",\"사망사고건수\",\"중상사고건수\",\"경상사고건수\"]\n",
    "is_safe = lambda c: not any(w in str(c) for w in LEAKS)\n",
    "\n",
    "cat_base = [c for c in CANDIDATE_CATS if c in df.columns and is_safe(c)]\n",
    "num_base = [c for c in CANDIDATE_NUMS if c in df.columns and is_safe(c)]\n",
    "\n",
    "# 희소 레벨 정리\n",
    "for c in cat_base:\n",
    "    df = collapse_rare_levels(df, c, min_n=MIN_LEVEL_N)\n",
    "\n",
    "# near-aliasing 제거\n",
    "cat_base, dropped_alias = drop_near_alias(df, cat_base, thresh=0.98)\n",
    "if dropped_alias:\n",
    "    print(\"[auto-drop aliasing]\", dropped_alias)\n",
    "\n",
    "# 상호작용 리스트 중 존재하는 것만 → 유효성 검사까지\n",
    "SELECTED_INTERACTIONS = [(a,b) for (a,b) in SELECTED_INTERACTIONS if (a in cat_base and b in cat_base)]\n",
    "SELECTED_INTERACTIONS = validate_and_filter_interactions(df, cat_base, SELECTED_INTERACTIONS)\n",
    "\n",
    "# 결측 대치\n",
    "df = impute_all(df, cat_base, num_base)\n",
    "\n",
    "# ========== 학습 표본 층화 샘플링 ==========\n",
    "df_train = stratified_cap(df, SELECTED_INTERACTIONS, cat_base, max_rows=MAX_TRAIN_ROWS, seed=RANDOM_SEED)\n",
    "print(f\"[sample] train rows: {len(df_train):,} / {len(df):,}\")\n",
    "\n",
    "# ========== 1) 베이스(주효과) 모형 ==========\n",
    "cluster_col = choose_cluster_col(df_train)\n",
    "cov_type0, cov_kw0 = cov_from(df_train, cluster_col)\n",
    "\n",
    "X_base = build_X(df_train, cat_base, num_base, inter_pairs=[])  # 상호작용 없음\n",
    "y_raw = df_train[TARGET].values\n",
    "X_base, y_base, cov_kw0 = align_and_clean(X_base, y_raw, cov_kw0)\n",
    "fit_base = fit_glm_gamma(X_base, y_base, cov_type0, cov_kw0)\n",
    "export_coef_table(fit_base, \"MODEL_BASE_results.csv\")\n",
    "print(f\"[BASE] AIC={fit_base.aic:.1f}, n={int(fit_base.nobs)} -> MODEL_BASE_results.csv\")\n",
    "\n",
    "# ========== 2) 풀(주효과 + 선택 상호작용) 모형 ==========\n",
    "cov_typeF, cov_kwF = cov_from(df_train, cluster_col)\n",
    "X_full = build_X(df_train, cat_base, num_base, inter_pairs=SELECTED_INTERACTIONS)\n",
    "X_full, y_full, cov_kwF = align_and_clean(X_full, y_raw, cov_kwF)\n",
    "fit_full = fit_glm_gamma(X_full, y_full, cov_typeF, cov_kwF)\n",
    "export_coef_table(fit_full, \"MODEL_FULL_results.csv\")\n",
    "pd.DataFrame({\"interaction_pairs\":[f\"{a} × {b}\" for a,b in SELECTED_INTERACTIONS]}).to_csv(\n",
    "    \"INTER_LIST_used.csv\", index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"[FULL] AIC={fit_full.aic:.1f}, n={int(fit_full.nobs)} -> MODEL_FULL_results.csv, INTER_LIST_used.csv\")\n",
    "\n",
    "# ========== 3) BASE vs FULL 개선도(LR 테스트 & ΔAIC/BIC) ==========\n",
    "ll0, ll1 = fit_base.llf, fit_full.llf\n",
    "df0, df1 = fit_base.df_model, fit_full.df_model\n",
    "LR = 2*(ll1 - ll0)\n",
    "ddf = max(int(df1 - df0), 1)\n",
    "p = chi2.sf(LR, ddf)\n",
    "delta_AIC = fit_full.aic - fit_base.aic\n",
    "delta_BIC = fit_full.bic - fit_base.bic\n",
    "\n",
    "pd.DataFrame([{\n",
    "    \"LR_stat\": LR, \"df_diff\": ddf, \"p_value\": p,\n",
    "    \"AIC_base\": fit_base.aic, \"AIC_full\": fit_full.aic, \"ΔAIC\": delta_AIC,\n",
    "    \"BIC_base\": fit_base.bic, \"BIC_full\": fit_full.bic, \"ΔBIC\": delta_BIC,\n",
    "    \"n_train\": int(fit_full.nobs)\n",
    "}]).to_csv(\"LR_TEST_BASE_vs_FULL.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"[COMPARE] LR_TEST_BASE_vs_FULL.csv saved\")\n",
    "print(f\"  LR={LR:.2f}, df={ddf}, p={p:.3e}, ΔAIC={delta_AIC:.2f}, ΔBIC={delta_BIC:.2f}\")\n",
    "\n",
    "# ========== 4) 대표 상호작용(첫 번째) 피벗 예측 ==========\n",
    "if SELECTED_INTERACTIONS:\n",
    "    a, b = SELECTED_INTERACTIONS[0]\n",
    "    # 대표값(연속=중앙값, 범주=최빈값)으로 다른 공변량 고정 (학습표본 기준)\n",
    "    rep = {}\n",
    "    for c in num_base:\n",
    "        if c in df_train.columns:\n",
    "            rep[c] = pd.to_numeric(df_train[c], errors=\"coerce\").median()\n",
    "            if pd.isna(rep[c]): rep[c] = 0.0\n",
    "    for c in cat_base:\n",
    "        if c not in [a,b] and c in df_train.columns:\n",
    "            mode_series = df_train[c].mode(dropna=True)\n",
    "            rep[c] = mode_series.iloc[0] if len(mode_series) else \"미상\"\n",
    "\n",
    "    # 그리드 만들기: a,b의 모든(TopK/캡 후) 레벨 조합\n",
    "    df_ab = df_train[[a,b]].copy()\n",
    "    df_ab = keep_topk_levels(df_ab, a, TOPK_LEVELS_INTER)\n",
    "    df_ab = keep_topk_levels(df_ab, b, TOPK_LEVELS_INTER)\n",
    "    df_ab = cap_interaction_levels_for_pair(df_ab, a, b, max_inter_dummies=MAX_INTER_DUMMIES)\n",
    "    A_levels = df_ab[a].dropna().unique().tolist()\n",
    "    B_levels = df_ab[b].dropna().unique().tolist()\n",
    "\n",
    "    grid = [{a: av, b: bv, **rep} for av in A_levels for bv in B_levels]\n",
    "    new = pd.DataFrame(grid, columns=uniq([a,b] + list(rep.keys())))\n",
    "\n",
    "    # 예측용 설계행렬(훈련 X_full과 동일 규칙 + 열정렬)\n",
    "    X_new = build_X(new, cat_base, num_base, inter_pairs=SELECTED_INTERACTIONS)\n",
    "    X_new = X_new.reindex(columns=fit_full.model.exog_names, fill_value=0.0)\n",
    "\n",
    "    mean, low, high = safe_predict_mean(fit_full, X_new)\n",
    "    new[\"pred_mean\"] = mean\n",
    "    if low is not None:  new[\"pred_low\"]  = low\n",
    "    if high is not None: new[\"pred_high\"] = high\n",
    "\n",
    "    # 피벗: 예측 평균\n",
    "    pivot = new.pivot(index=a, columns=b, values=\"pred_mean\")\n",
    "    pivot.to_csv(f\"PIVOT_pred_mean_{a}x{b}.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "    # 배수효과(%): pivot의 [0,0] 기준\n",
    "    base_val = pivot.iloc[0,0]\n",
    "    mult = (pivot / base_val - 1.0) * 100.0\n",
    "    mult.to_csv(f\"PIVOT_mult_%_{a}x{b}.csv\", encoding=\"utf-8-sig\")\n",
    "    print(f\"[PIVOT] saved -> PIVOT_pred_mean_{a}x{b}.csv, PIVOT_mult_%_{a}x{b}.csv\")\n",
    "else:\n",
    "    print(\"[PIVOT] 상호작용이 지정되지 않아 피벗 생략\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
